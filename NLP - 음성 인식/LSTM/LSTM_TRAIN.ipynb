{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "from torch.utils.data.dataset import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Kkma\n",
    "kkma = Kkma()\n",
    "\n",
    "ko_model = gensim.models.Word2Vec.load('./ko/ko.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Komoran\n",
    "komoran = Komoran()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetDataSet(Dataset):\n",
    "    def __init__(self,csv_file):\n",
    "        self.Target = pd.read_csv(csv_file,sep = '\\t',header = None,error_bad_lines=False)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.Target)\n",
    "    def __getitem__(self, idx):\n",
    "        data = []\n",
    "        texts = self.Target[0][idx]\n",
    "        sen_split = komoran.morphs(texts)\n",
    "#         print(sen_split)\n",
    "        w2v_list = []\n",
    "        for i in range(20):\n",
    "            if len(sen_split)>i:\n",
    "                try:\n",
    "                    w2v = ko_model.wv[sen_split[i]]\n",
    "                    w2v_list.append(w2v)\n",
    "                except:\n",
    "                    w2v_list.append(np.zeros(200))\n",
    "            else:\n",
    "                w2v_list.append(np.zeros(200))\n",
    "            \n",
    "        w2v_list = np.array(w2v_list)\n",
    "        data.append(w2v_list)\n",
    "        data = np.array(data)\n",
    "        data = data.reshape(20,200)\n",
    "        data = torch.from_numpy(data.astype('float32'))\n",
    "        label = torch.from_numpy(np.array(self.Target[1][idx]))\n",
    "        \n",
    "        return data,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 79: expected 2 fields, saw 3\\nSkipping line 175: expected 2 fields, saw 3\\n'\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "\n",
    "TARGET_DATASET_TRAIN = TargetDataSet(csv_file = './data/traindata.txt')\n",
    "# TARGET_DATASET_VAL   = TargetDataSet(csv_file = 'traindata.txt')\n",
    "\n",
    "TARGET_DATASET_TRAIN_LODER = torch.utils.data.DataLoader(TARGET_DATASET_TRAIN, batch_size = batch_size, shuffle = True, num_workers = 0)\n",
    "# TARGET_DATASET_VAL_LODER = torch.utils.data.DataLoader(TARGET_DATASET_TRAIN, batch_size = batch_sizem shuffle = True, num_workers = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.3369, -0.1450,  0.3567,  ..., -0.2557,  0.6565,  0.0740],\n",
       "         [-1.2767,  1.1824, -1.2436,  ...,  1.3085,  0.0848, -1.5699],\n",
       "         [ 1.3817,  1.0780,  1.9430,  ...,  2.1113, -2.5899,  1.2127],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]),\n",
       " tensor(1))"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TARGET_DATASET_TRAIN[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 200])"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TARGET_DATASET_TRAIN[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 20, 200])"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataiter = iter(TARGET_DATASET_TRAIN_LODER)\n",
    "vector, labels = dataiter.next()\n",
    "# print(vector,labels)\n",
    "vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size  = 200\n",
    "hidden_size = 1\n",
    "batch_size  = 4\n",
    "seq_len     = 20\n",
    "\n",
    "num_layer   = 1\n",
    "num_classes = 2\n",
    "\n",
    "\n",
    "learning_rate = 0.001\n",
    "#==================Test용 input hidden입니다============================\n",
    "inputs  = Variable(torch.randn(batch_size,seq_len,input_size))\n",
    "hidden = Variable(torch.zeros(1,batch_size,hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Set initial hidden and cell states \n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        \n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return F.softmax(out,dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = False\n",
    "if(check == True):\n",
    "    model = RNN(input_size, hidden_size, num_layer, num_classes)\n",
    "    model.load_state_dict(torch.load(\"T2W\"))\n",
    "else:\n",
    "    model = RNN(input_size, hidden_size, num_layer, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.train()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "for epoch in range(100):\n",
    "    TRN_LOSS = 0\n",
    "    for i,data in enumerate(TARGET_DATASET_TRAIN_LODER,0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = Variable(inputs),Variable(labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model.forward(inputs)\n",
    "        loss   = criterion(output,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        TRN_LOSS +=loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"T2W\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['엄마', '가', '다치', '시', '었', '는데', '걱정', '하', '지', '말', '고', '아줌마', '랑', '가자']"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = '엄마가 다치셨는데 걱정하지 말고 아줌마랑 가자'\n",
    "# sen_split = kkma.morphs(texts)\n",
    "sen_split = komoran.morphs(texts)\n",
    "sen_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1929, -0.6049,  1.5089,  ...,  0.0597,  0.9609, -1.1660],\n",
       "         [-2.4270, -1.5736,  0.8362,  ...,  0.4343,  1.4763, -0.1952],\n",
       "         [ 0.1413, -0.3792, -0.1403,  ..., -0.7748,  0.6649, -0.6575],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]])"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#=========나중에 실시간으로 들어오는거 처리할때 무조건 필요하니까 절대 지우면 안된다!!!=========\n",
    "\n",
    "\n",
    "w2v_list = []\n",
    "for i in range(20):\n",
    "    if len(sen_split)>i:\n",
    "        print(len(sen_split))\n",
    "        try:\n",
    "            w2v = ko_model.wv[sen_split[i]]\n",
    "            w2v_list.append(w2v)\n",
    "        except:\n",
    "            print(\"asd\")\n",
    "            w2v_list.append(np.zeros(200))\n",
    "    else:\n",
    "#         print(\"asd\")\n",
    "        w2v_list.append(np.zeros(200))\n",
    "    \n",
    "w2v_list = np.array(w2v_list)\n",
    "w2v_tensor    = torch.from_numpy(w2v_list.astype('float32'))\n",
    "w2v_tensor_ch =torch.reshape(w2v_tensor,(1,w2v_tensor.size()[0],w2v_tensor.size()[1]))\n",
    "w2v_tensor_ch.size()\n",
    "w2v_tensor_ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0221, 0.9779]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AA = model(w2v_tensor_ch)\n",
    "AA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 정확도 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Train_Accuracy of the model: 1.0\n"
     ]
    }
   ],
   "source": [
    "#test the model\n",
    "model.eval()\n",
    "train_correct = 0\n",
    "train_total = 0\n",
    "\n",
    "train_prediction_list = []\n",
    "train_label_list = []\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i,data in enumerate(TARGET_DATASET_TRAIN_LODER,0):\n",
    "        train_data, train_label = data\n",
    "        print()\n",
    "        train_output = model.forward(train_data)\n",
    "        _, train_prediction_index = torch.max(train_output, 1)\n",
    "        \n",
    "        train_prediction_list.append(train_prediction_index)\n",
    "        train_label_list.append(train_label)\n",
    "        \n",
    "        train_total += train_label.size(0)\n",
    "        train_correct += (train_prediction_index == train_label).sum().float()\n",
    "    \n",
    "\n",
    "    print(\"Train_Accuracy of the model: {}\".format(train_correct/train_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 이 밑에 지우면 너가 죽는다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetDataSet(Dataset):\n",
    "    def __init__(self,csv_file):\n",
    "        self.Target = pd.read_csv(csv_file,sep = '\\t',header = None,error_bad_lines=False)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.Target)\n",
    "    def __getitem__(self, idx):\n",
    "        data = []\n",
    "        for sen_split in self.Target[0]:\n",
    "            w2v_list = []\n",
    "            for i in range(30):\n",
    "#                 print(i)\n",
    "                if len(sen_split)>i:\n",
    "                    try:\n",
    "                        w2v = ko_model.wv[sen_split[i]]\n",
    "                        w2v_list.append(w2v)\n",
    "                    except:\n",
    "                        w2v_list.append(np.zeros(200))\n",
    "                else:\n",
    "                    w2v_list.append(np.zeros(200))\n",
    "            \n",
    "            w2v_list = np.array(w2v_list)\n",
    "            data.append(w2v_list)\n",
    "            \n",
    "        data = np.array(data)\n",
    "        data = torch.from_numpy(data.astype('float32'))\n",
    "        \n",
    "        label = torch.from_numpy(np.array(self.Target[1]))\n",
    "        \n",
    "        return data,label"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
